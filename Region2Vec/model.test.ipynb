{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "Vector_size = 256\n",
    "Top_Hit = 50\n",
    "Max_Demaged_Image = 5\n",
    "\n",
    "\n",
    "def extract_text(text):\n",
    "    parts = text.split(\":\")\n",
    "    if len(parts) > 1:\n",
    "        return parts[1].strip()\n",
    "    else:\n",
    "        return text.strip()\n",
    "    \n",
    "\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "\n",
    "meta_file_path = \"Meta.txt\"\n",
    "\n",
    "\n",
    "word_vectors = {}\n",
    "\n",
    "\n",
    "with open(meta_file_path, 'r', encoding='utf-8') as meta_file:\n",
    "    lines = meta_file.readlines()\n",
    "\n",
    "cnt = 0\n",
    "for line in lines:\n",
    "    extracted_text = extract_text(line)\n",
    "    words_in_line = extracted_text.split()\n",
    "    for word in words_in_line:\n",
    "        if word in model.wv:\n",
    "            vector = model.wv[word]\n",
    "           \n",
    "        else :\n",
    "            vector = np.zeros(Vector_size,)\n",
    "        word_vectors[cnt] = vector\n",
    "        cnt += 1\n",
    "\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(word_vectors.values())).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vectors = np.array(list(word_vectors.values())).reshape(cnt, Vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta_file_path = \"Meta.txt\"\n",
    "\n",
    "word_vectors = {}\n",
    "\n",
    "# Meta.txt 파일을 엽니다.\n",
    "with open(meta_file_path, 'r', encoding='utf-8') as meta_file:\n",
    "    lines = meta_file.readlines()\n",
    "\n",
    "\n",
    "word_list=[]\n",
    "for line in lines:\n",
    "    extracted_text = extract_text(line)\n",
    "    words_in_line = extracted_text.split()\n",
    "    for word in words_in_line:\n",
    "        word_list.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "json_file_path = \"roadsign_data_info_damaged_sample_0928_수정.json\"\n",
    "\n",
    "\n",
    "with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "\n",
    "annotations = data.get(\"annotations\", [])\n",
    "\n",
    "\n",
    "bbox_texts_by_image_id = {}\n",
    "\n",
    "#print(annotations)\n",
    "\n",
    "\n",
    "for annotation in annotations:\n",
    "    damaged_status = annotation.get(\"damaged\")\n",
    "    bbox_text = annotation.get(\"bbox_text\")\n",
    "    image_id = annotation.get(\"image_id\")\n",
    "    \n",
    "\n",
    "    if image_id not in bbox_texts_by_image_id:\n",
    "        bbox_texts_by_image_id[image_id] = {\"N\": [], \"Y\": []}\n",
    "    \n",
    "    if damaged_status == \"N\":\n",
    "        bbox_texts_by_image_id[image_id][\"N\"].append(bbox_text)\n",
    "    elif damaged_status == \"Y\":\n",
    "        bbox_texts_by_image_id[image_id][\"Y\"].append(bbox_text)\n",
    "\n",
    "\n",
    "res = []\n",
    "bbox_N_idx = 0\n",
    "try_count = 0\n",
    "for image_id, bbox_texts in bbox_texts_by_image_id.items():\n",
    "    \n",
    "    print(\"Image ID:\", image_id)\n",
    "    \n",
    "    res_image_id = []\n",
    "\n",
    "    print(\"['damaged':'N'] Bbox Texts:\")\n",
    "    for bbox_text in bbox_texts[\"N\"]:\n",
    "        if bbox_text in model.wv:\n",
    "            vector = model.wv[bbox_text]\n",
    "            reshape_vector = vector.reshape(1, Vector_size)\n",
    "            res_image_id.append(cosine_similarity(reshape_vector,all_vectors))\n",
    "            # print(res)\n",
    "            # print(f\"{bbox_text}: {vector}\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"{bbox_text} is not in the vocabulary.\")\n",
    "            res_image_id.append(np.zeros(cnt))\n",
    "    \n",
    "    res.append(res_image_id)\n",
    "    \n",
    "    #============================================================\n",
    "    \n",
    "    bbox_idx = 0\n",
    "    \n",
    "\n",
    "    if bbox_texts[\"N\"] == [] :\n",
    "        print(\"N x\")\n",
    "        print()\n",
    "        continue\n",
    "    for bbox_text in bbox_texts[\"N\"]:\n",
    "\n",
    "        temp = res[int(image_id)-1][bbox_idx].reshape(cnt)\n",
    "        indices = np.argsort(temp)\n",
    "        hit_step = 1\n",
    "\n",
    "\n",
    "        if temp[0] == 0:\n",
    "            # print(f\"{bbox_text} is not in the vocabulary.\",bbox_idx)\n",
    "            bbox_idx+=1\n",
    "            continue\n",
    "        # N이 정해짐\n",
    "        # Y를 정할 차례\n",
    "\n",
    "        if bbox_texts[\"Y\"] == []:\n",
    "            hit_step += Top_Hit\n",
    "            break\n",
    "            \n",
    "        for bbox_text_Y in bbox_texts[\"Y\"]:\n",
    "            for idx in reversed(indices[-Top_Hit:]):\n",
    "                if word_list[idx] == bbox_text_Y: \n",
    "                    break \n",
    "                hit_step +=1\n",
    "  \n",
    "            # if break_point == True:\n",
    "            #     break_point = False\n",
    "            #     break\n",
    "            if hit_step > Top_Hit: print(f\"Hit fails when '{bbox_text}'\")\n",
    "            else: \n",
    "                print(f\"When '{bbox_text}' hit form\",hit_step) \n",
    "                break\n",
    "        # try_count+=1\n",
    "        \n",
    "        bbox_idx+=1\n",
    "    try_count += len(bbox_texts[\"Y\"])\n",
    "    \n",
    "    #============================================================\n",
    "    print(\"['damaged':'Y'] Bbox Texts:\", bbox_texts[\"Y\"])\n",
    "    print()\n",
    "print(try_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Y먼저 N나중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_Hit = 50\n",
    "\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "\n",
    "json_file_path = \"roadsign_data_info_damaged_sample_0928_수정.json\"\n",
    "\n",
    "\n",
    "with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "\n",
    "annotations = data.get(\"annotations\", [])\n",
    "\n",
    "\n",
    "bbox_texts_by_image_id = {}\n",
    "\n",
    "\n",
    "for annotation in annotations:\n",
    "    damaged_status = annotation.get(\"damaged\")\n",
    "    bbox_text = annotation.get(\"bbox_text\")\n",
    "    image_id = annotation.get(\"image_id\")\n",
    "    \n",
    "\n",
    "    if image_id not in bbox_texts_by_image_id:\n",
    "        bbox_texts_by_image_id[image_id] = {\"N\": [], \"Y\": []}\n",
    "    \n",
    "    if damaged_status == \"N\":\n",
    "        bbox_texts_by_image_id[image_id][\"N\"].append(bbox_text)\n",
    "    elif damaged_status == \"Y\":\n",
    "        bbox_texts_by_image_id[image_id][\"Y\"].append(bbox_text)\n",
    "\n",
    "\n",
    "res = []\n",
    "bbox_N_idx = 0\n",
    "try_count = 0\n",
    "hit_count = 0\n",
    "for image_id, bbox_texts in bbox_texts_by_image_id.items():\n",
    "    \n",
    "    print(\"Image ID:\", image_id)\n",
    "    \n",
    "    res_image_id = []\n",
    "\n",
    "    print(\"['damaged':'N'] Bbox Texts:\")\n",
    "    for bbox_text in bbox_texts[\"N\"]:\n",
    "        if bbox_text in model.wv:\n",
    "            vector = model.wv[bbox_text]\n",
    "            reshape_vector = vector.reshape(1, Vector_size)\n",
    "            res_image_id.append(cosine_similarity(reshape_vector,all_vectors))\n",
    "            # print(res)\n",
    "            # print(f\"{bbox_text}: {vector}\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"{bbox_text} is not in the vocabulary.\")\n",
    "            res_image_id.append(np.zeros(cnt))\n",
    "    \n",
    "    res.append(res_image_id)\n",
    "    \n",
    "    #============================================================\n",
    "    \n",
    "    \n",
    "    \n",
    "    # if bbox_texts[\"Y\"] == []:\n",
    "    #     hit_step += Top_Hit\n",
    "            \n",
    "            \n",
    "    for bbox_text_Y in bbox_texts[\"Y\"]:\n",
    "        # Y이 정해짐\n",
    "        # N를 정할 차례\n",
    "\n",
    "        if bbox_texts[\"N\"] == [] :\n",
    "            print(\"N x\")\n",
    "            print()\n",
    "            break\n",
    "\n",
    "        bbox_idx = 0\n",
    "        for bbox_text in bbox_texts[\"N\"]:\n",
    "            temp = res[int(image_id)-1][bbox_idx].reshape(cnt)\n",
    "            indices = np.argsort(temp)\n",
    "            if temp[0] == 0:\n",
    "                # print(f\"{bbox_text} is not in the vocabulary.\",bbox_idx)\n",
    "                bbox_idx+=1\n",
    "                continue\n",
    "            hit_step = 1\n",
    "            for idx in reversed(indices[-Top_Hit:]):\n",
    "                if word_list[idx] == bbox_text_Y: \n",
    "                    break \n",
    "                hit_step +=1\n",
    "            bbox_idx+=1\n",
    "\n",
    "            if hit_step > Top_Hit: print(f\"Hit fails when '{bbox_text}'\")\n",
    "            else: \n",
    "                hit_count +=1\n",
    "                print(f\"When '{bbox_text}' hit from\",hit_step) \n",
    "                break\n",
    "        try_count+=1\n",
    "\n",
    "    #============================================================\n",
    "    print(\"['damaged':'Y'] Bbox Texts:\", bbox_texts[\"Y\"])\n",
    "    print()\n",
    "\n",
    "print(try_count)\n",
    "print(hit_count/try_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp = res[7][0].reshape(cnt)\n",
    "#print(temp)\n",
    "#temp = res[image_id][bbox_idx].reshape(cnt)\n",
    "indices = np.argsort(temp)\n",
    "print(indices)\n",
    "\n",
    "for idx in indices[-12:]:\n",
    "    print(word_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_Hit = 100\n",
    "\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "\n",
    "json_file_path = \"roadsign_data_info_damaged_sample_0928_수정.json\"\n",
    "\n",
    "with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "\n",
    "annotations = data.get(\"annotations\", [])\n",
    "\n",
    "\n",
    "bbox_texts_by_image_id = {}\n",
    "\n",
    "\n",
    "for annotation in annotations:\n",
    "    damaged_status = annotation.get(\"damaged\")\n",
    "    bbox_text = annotation.get(\"bbox_text\")\n",
    "    image_id = annotation.get(\"image_id\")\n",
    "    \n",
    "\n",
    "    if image_id not in bbox_texts_by_image_id:\n",
    "        bbox_texts_by_image_id[image_id] = {\"N\": [], \"Y\": []}\n",
    "    \n",
    "    if damaged_status == \"N\":\n",
    "        bbox_texts_by_image_id[image_id][\"N\"].append(bbox_text)\n",
    "    elif damaged_status == \"Y\":\n",
    "        bbox_texts_by_image_id[image_id][\"Y\"].append(bbox_text)\n",
    "\n",
    "\n",
    "res = []\n",
    "bbox_N_idx = 0\n",
    "try_count = 0\n",
    "hit_count = 0\n",
    "for image_id, bbox_texts in bbox_texts_by_image_id.items():\n",
    "    \n",
    "    print(\"Image ID:\", image_id)\n",
    "    \n",
    "    res_image_id = []\n",
    "\n",
    "    for bbox_text in bbox_texts[\"N\"]:\n",
    "        if bbox_text in model.wv:\n",
    "            vector = model.wv[bbox_text]\n",
    "            reshape_vector = vector.reshape(1, Vector_size)\n",
    "            res_image_id.append(cosine_similarity(reshape_vector,all_vectors))\n",
    "            # print(res)\n",
    "            # print(f\"{bbox_text}: {vector}\")\n",
    "        \n",
    "        else:\n",
    "            # print(f\"{bbox_text} is not in the vocabulary.\")\n",
    "            res_image_id.append(np.zeros(cnt))\n",
    "    \n",
    "    res.append(res_image_id)\n",
    "    \n",
    "    #============================================================\n",
    "    \n",
    "    \n",
    "    \n",
    "    # if bbox_texts[\"Y\"] == []:\n",
    "    #     hit_step += Top_Hit\n",
    "            \n",
    "            \n",
    "    for bbox_text_Y in bbox_texts[\"Y\"]:\n",
    "        # Y이 정해짐\n",
    "        # N를 정할 차례\n",
    "\n",
    "        if bbox_texts[\"N\"] == [] :\n",
    "            # print(\"N x\")\n",
    "            # print()\n",
    "            break\n",
    "\n",
    "        bbox_idx = 0\n",
    "        for bbox_text in bbox_texts[\"N\"]:\n",
    "            temp = res[int(image_id)-1][bbox_idx].reshape(cnt)\n",
    "            indices = np.argsort(temp)\n",
    "            if temp[0] == 0:\n",
    "                # print(f\"{bbox_text} is not in the vocabulary.\",bbox_idx)\n",
    "                bbox_idx+=1\n",
    "                continue\n",
    "            hit_step = 1\n",
    "            print(f\"{bbox_text} : \",end='')\n",
    "            for idx in reversed(indices[-Top_Hit:]):\n",
    "                print(word_list[idx], ' ',end ='')\n",
    "            bbox_idx+=1\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_Hit = 100\n",
    "\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "\n",
    "json_file_path = \"roadsign_data_info_damaged_sample_0928_수정.json\"\n",
    "\n",
    "with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "\n",
    "annotations = data.get(\"annotations\", [])\n",
    "\n",
    "\n",
    "bbox_texts_by_image_id = {}\n",
    "\n",
    "\n",
    "for annotation in annotations:\n",
    "    damaged_status = annotation.get(\"damaged\")\n",
    "    bbox_text = annotation.get(\"bbox_text\")\n",
    "    image_id = annotation.get(\"image_id\")\n",
    "    \n",
    "\n",
    "    if image_id not in bbox_texts_by_image_id:\n",
    "        bbox_texts_by_image_id[image_id] = {\"N\": [], \"Y\": []}\n",
    "    \n",
    "    if damaged_status == \"N\":\n",
    "        bbox_texts_by_image_id[image_id][\"N\"].append(bbox_text)\n",
    "    elif damaged_status == \"Y\":\n",
    "        bbox_texts_by_image_id[image_id][\"Y\"].append(bbox_text)\n",
    "\n",
    "\n",
    "res = []\n",
    "bbox_N_idx = 0\n",
    "try_count = 0\n",
    "hit_count = 0\n",
    "\n",
    "for image_id, bbox_texts in bbox_texts_by_image_id.items():\n",
    "    \n",
    "    print(\"Image ID:\", image_id)\n",
    "    \n",
    "    res_image_id = []\n",
    "\n",
    "    for bbox_text in bbox_texts[\"N\"]:\n",
    "        if bbox_text in model.wv:\n",
    "            vector = model.wv[bbox_text]\n",
    "            reshape_vector = vector.reshape(1, Vector_size)\n",
    "            res_image_id.append(cosine_similarity(reshape_vector,all_vectors))\n",
    "            # print(res)\n",
    "            # print(f\"{bbox_text}: {vector}\")\n",
    "        \n",
    "        else:\n",
    "            # print(f\"{bbox_text} is not in the vocabulary.\")\n",
    "            res_image_id.append(np.zeros(cnt))\n",
    "    \n",
    "    res.append(res_image_id)\n",
    "    \n",
    "    #============================================================\n",
    "    \n",
    "    \n",
    "    \n",
    "    # if bbox_texts[\"Y\"] == []:\n",
    "    #     hit_step += Top_Hit\n",
    "            \n",
    "            \n",
    "    for bbox_text_Y in bbox_texts[\"Y\"]:\n",
    "        # Y이 정해짐\n",
    "        # N를 정할 차례\n",
    "\n",
    "        if bbox_texts[\"N\"] == [] :\n",
    "            # print(\"N x\")\n",
    "            # print()\n",
    "            break\n",
    "\n",
    "        bbox_idx = 0\n",
    "        for bbox_text in bbox_texts[\"N\"]:\n",
    "            temp = res[int(image_id)-1][bbox_idx].reshape(cnt)\n",
    "            indices = np.argsort(temp)\n",
    "            res_Array = res[int(image_id)-1][bbox_idx].reshape(cnt)\n",
    "            if temp[0] == 0:\n",
    "                # print(f\"{bbox_text} is not in the vocabulary.\",bbox_idx)\n",
    "                bbox_idx+=1\n",
    "                continue\n",
    "            hit_step = 1\n",
    "            print(f\"{bbox_text} : \",end='')\n",
    "            for idx in reversed(indices[-Top_Hit:]):\n",
    "                print(\"<\",word_list[idx], res_Array[idx],\">\",' ',end ='')\n",
    "            bbox_idx+=1\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_Hit = 100\n",
    "\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "\n",
    "json_file_path = \"roadsign_data_info_damaged_sample_0928_수정.json\"\n",
    "\n",
    "with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "\n",
    "annotations = data.get(\"annotations\", [])\n",
    "\n",
    "\n",
    "bbox_texts_by_image_id = {}\n",
    "\n",
    "\n",
    "for annotation in annotations:\n",
    "    damaged_status = annotation.get(\"damaged\")\n",
    "    bbox_text = annotation.get(\"bbox_text\")\n",
    "    image_id = annotation.get(\"image_id\")\n",
    "    \n",
    "\n",
    "    if image_id not in bbox_texts_by_image_id:\n",
    "        bbox_texts_by_image_id[image_id] = {\"N\": [], \"Y\": []}\n",
    "    \n",
    "    if damaged_status == \"N\":\n",
    "        bbox_texts_by_image_id[image_id][\"N\"].append(bbox_text)\n",
    "    elif damaged_status == \"Y\":\n",
    "        bbox_texts_by_image_id[image_id][\"Y\"].append(bbox_text)\n",
    "\n",
    "\n",
    "res = []\n",
    "bbox_N_idx = 0\n",
    "try_count = 0\n",
    "hit_count = 0\n",
    "for image_id, bbox_texts in bbox_texts_by_image_id.items():\n",
    "    \n",
    "    print(\"Image ID:\", image_id)\n",
    "    \n",
    "    res_image_id = []\n",
    "\n",
    "    for bbox_text in bbox_texts[\"N\"]:\n",
    "        if bbox_text in model.wv:\n",
    "            vector = model.wv[bbox_text]\n",
    "            reshape_vector = vector.reshape(1, Vector_size)\n",
    "            res_image_id.append(cosine_similarity(reshape_vector,all_vectors))\n",
    "            # print(res)\n",
    "            print(f\"{bbox_text}: {vector}\")\n",
    "        \n",
    "        else:\n",
    "            # print(f\"{bbox_text} is not in the vocabulary.\")\n",
    "            res_image_id.append(np.zeros(cnt))\n",
    "    \n",
    "    res.append(res_image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_Hit = 100\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "json_file_path = \"roadsign_data_info_damaged_sample_0530_수정.json\"\n",
    "\n",
    "with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "annotations = data.get(\"annotations\", [])\n",
    "\n",
    "bbox_texts_by_image_id = {}\n",
    "\n",
    "for annotation in annotations:\n",
    "    damaged_status = annotation.get(\"damaged\")\n",
    "    bbox_text = annotation.get(\"bbox_text\")\n",
    "    image_id = annotation.get(\"image_id\")\n",
    "    \n",
    " \n",
    "    if image_id not in bbox_texts_by_image_id:\n",
    "        bbox_texts_by_image_id[image_id] = {\"N\": [], \"Y\": []}\n",
    "    \n",
    "    if damaged_status == \"N\":\n",
    "        bbox_texts_by_image_id[image_id][\"N\"].append(bbox_text)\n",
    "    elif damaged_status == \"Y\":\n",
    "        bbox_texts_by_image_id[image_id][\"Y\"].append(bbox_text)\n",
    "\n",
    "\n",
    "for Top_Hit in range(1,101):\n",
    "    res = []\n",
    "    bbox_N_idx = 0\n",
    "    try_count = 0\n",
    "    hit_count = 0\n",
    "    for image_id, bbox_texts in bbox_texts_by_image_id.items():\n",
    "        \n",
    "        # print(\"Image ID:\", image_id)\n",
    "        \n",
    "        res_image_id = []\n",
    "        \n",
    "        # print(\"['damaged':'N'] Bbox Texts:\")\n",
    "        for bbox_text in bbox_texts[\"N\"]:\n",
    "            if bbox_text in model.wv:\n",
    "                vector = model.wv[bbox_text]\n",
    "                reshape_vector = vector.reshape(1, Vector_size)\n",
    "                res_image_id.append(cosine_similarity(reshape_vector,all_vectors))\n",
    "                # print(res)\n",
    "                # print(f\"{bbox_text}: {vector}\")\n",
    "            \n",
    "            else:\n",
    "                # print(f\"{bbox_text} is not in the vocabulary.\")\n",
    "                res_image_id.append(np.zeros(cnt))\n",
    "        \n",
    "        res.append(res_image_id)\n",
    "        \n",
    "        #============================================================\n",
    "        \n",
    "        \n",
    "        \n",
    "        # if bbox_texts[\"Y\"] == []:\n",
    "        #     hit_step += Top_Hit\n",
    "                \n",
    "                \n",
    "        for bbox_text_Y in bbox_texts[\"Y\"]:\n",
    "            # Y이 정해짐\n",
    "            # N를 정할 차례\n",
    "\n",
    "            if bbox_texts[\"N\"] == [] :\n",
    "                break\n",
    "\n",
    "            bbox_idx = 0\n",
    "            for bbox_text in bbox_texts[\"N\"]:\n",
    "                temp = res[int(image_id)-1][bbox_idx].reshape(cnt)\n",
    "                indices = np.argsort(temp)\n",
    "                if temp[0] == 0:\n",
    "                    # print(f\"{bbox_text} is not in the vocabulary.\",bbox_idx)\n",
    "                    bbox_idx+=1\n",
    "                    continue\n",
    "                hit_step = 1\n",
    "                for idx in reversed(indices[-Top_Hit:]):\n",
    "                    if word_list[idx] == bbox_text_Y: \n",
    "                        break \n",
    "                    hit_step +=1\n",
    "                bbox_idx+=1\n",
    "\n",
    "                if hit_step > Top_Hit: pass # print(f\"Hit fails when '{bbox_text}'\")\n",
    "                else: \n",
    "                    hit_count +=1\n",
    "                    # print(f\"When '{bbox_text}' hit from\",hit_step) \n",
    "                    break\n",
    "            try_count+=1\n",
    "\n",
    "        #============================================================\n",
    "        # print(\"['damaged':'Y'] Bbox Texts:\", bbox_texts[\"Y\"])\n",
    "        # print()\n",
    "    # print(try_count)\n",
    "    print(\"Top_hit : \" ,Top_Hit , \" hit rate : \" ,hit_count/try_count)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Region2Vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
